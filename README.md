# About Repository
This repo includes technical documentation and Python codebase for data preparation for 3D brain MR image classification using ADNI dataset.

We have used this data preprocessing and processing of ADNI dataset for several publications (see: [About Citation](https://github.com/djrumala/ADNI-processing#about-citation))

## Project Status
✅ **REFACTORED**: Converted from Jupyter Notebooks to modular Python codebase
- Clean package structure with reusable modules
- Executable scripts for each pipeline step
- Comprehensive logging and configuration
- Full documentation in [STRUCTURE.md](STRUCTURE.md)

# Steps 
The whole data preparation process is started out by:
1. [Data Collecting](https://github.com/djrumala/ADNI-processing#data-collecting)
2. [Data Cleaning](https://github.com/djrumala/ADNI-processingp#data-cleaning)
3. [Data Preprocessing](https://github.com/djrumala/ADNI-processing#data-preprocessing)
4. [Data Augmentation](https://github.com/djrumala/ADNI-processing#data-augmentation)
5. [Data Loading](https://github.com/djrumala/ADNI-processing#data-loading)

# Project Structure
```
ADNI-processing/
├── src/                      # Core Python package
│   ├── config.py            # Configuration & constants
│   ├── metadata.py          # Metadata utilities
│   ├── file_operations.py   # File moving functions
│   └── logging.py           # Logging utilities
│
├── scripts/                 # Executable workflows
│   ├── move_preprocessed_files.py
│   ├── move_to_preprocess.py
│   ├── move_to_convert.py
│   ├── move_final_files.py
│   └── run_pipeline.py      # Master orchestrator
│
├── outputs/                 # Results & logs
│   └── logs/
│
├── TempMeta/                # Metadata CSVs
├── TempData/                # Temporary data
├── preprocessed/            # Final preprocessed files
├── final/                   # Final output
│
└── STRUCTURE.md             # Detailed structure documentation
```

See [STRUCTURE.md](STRUCTURE.md) for complete details on the Python codebase structure, module reference, and usage examples.

# Data Collecting
The datasets are taken and downloaded from ADNI dataset here: `https://ida.loni.usc.edu/login.jsp`

Multispectral MRI datasets with 3 Tesla (3T) field strength of T1-weighted and T2-weighted are collected from ADNI1.

The original metadata of the downloaded files are saved inside the folder `/Metadata/` as `{seq}w_{group}_ADNI1_3T_{data download}.csv`


# Data Cleaning
Data cleaning follows several steps:
### 1. Generating list of relevant weighted MRI data 
The first data cleaning is to generate metadata of relevant MRI sequences from the original metadata.

As we know, MRI can generate more than one sequence (multi-spectral images) during a scan. In this case, according to [ADNI 1 Technical Procedures Manual](https://adni.loni.usc.edu/wp-content/uploads/2010/09/ADNI_MRI_Tech_Proc_Manual.pdf), when visitting for a scan, a subject will be asked for scans with the approved ADNI sequences. Two of the sequences include T1 (MP-RAGE) and T2 Dual Echo.

Relevant metadata of T1 is obtained by including only `MP-RAGE` from the `Description` column, and only `TSE/FSE` for T2. There is no use for filterring out certain visit times on `visit` column (in which such filter was used the first time to easily balance the data size for each group. And doing so will be hard to get pair of matched T1 and T2 that will be performed in Step 2. More details see the first but unused source code: `data_clean.ipynb`). 

Generated output from this process is saved inside the folder `/TempMeta/` with the file name `Cleaned_Ori_Nolimit_{seq}w_{group}_ADNI1_{tesla}T.csv.csv`

Source code: `data_cleaning_nolimit.ipynb`

### 2. Listing the pair of matched T1 and T2

We want to list the pair of matched T1 and T2 MRI data, meaning they are scans that come from one subject on a certain visit. 

In this step, coarse list of matched data is generated by comparing the values of several attributes (subject id, visit, and acquisition date) from the metadata of T1 and T2 data. Generated metadata is saved inside the folder `/TempMeta/` with the filename `Matched_Nolimit_{seq}w_{group}_ADNI1_3T_12_05_2022.csv`

However, generated metadata from this process still contains duplicated values. The counting of number of subjects and images generated from this list is performed in excel `Metadata_Matched_Nolimit.xlsx` on section `NotUnique_Count`

Source code: `data_matching.ipynb` (1st section)

### 3. Generate unique and balanced list of matched data

There are two generated list from this process:

1. Unique version of matched metadata in T1 and T2 (there are some missing data from T1, creating unbalanced list of matched data in T1 and T2)
2. Balanced version of matched metadata between T1 and T2 

More details are noted in the code.

Source code: `data_matching.ipynb` (2nd section)

### 4. Balancing data size of all groups
The process is performed manually in Excel. The intention is we want to balance the number of data of all groups both in T1 and T2. The result of this process is a metadata saved inside the folder of `/TempMeta/` with the filename `Balanced_Meta_{seq}w_{group}.csv`

The analysis of this process is available in Excel file: `Metadata_Balanced.xlsx`

### 5. Collecting and moving the to-be-preprocessed MRI data

There are three major steps in here:
1. Find and move the already preprocessed files and create list of not yet preprocessed files.
    - Compare the availability of the preprocessed files in folder `/preprocessed_old` with the generated final metadata in Step 4
    - The preprocessed files are moved to folder `/preprocessed/` with the filename `{meta id}-{original filename}.nii.gz.`
    - List of not yet preprocessed files is saved as metadata inside the folder `/TempMeta/` with the filename `To-Be-Preprocessed_{seq}w_{group}.csv`
    - Note: This step can be skipped if there is not yet preprocessed data, but might be useful later when there are missed preprocessed data during the preprocessing step.
2. Moving the not yet preprocessed files
    - Path of the source files refers to the raw original directory. In this work, raw directory is `/3T/`
    - Comparing files available in the original directory with the generated metadata in step 5.1. 
    - The files will be moved to folder `/TempData/` and put inside a decent sub-directory that contains `{subject id}-{series id}-{image id}`
    - The files will be kept as its original filename
3. Moving the new preprocessed files
    - Compare the availability of the additional preprocessed files in folder `/preprocessed_addition` with the generated final metadata in Step 4
    - The preprocessed files are moved to folder `/preprocessed/` with the filename `{meta id}-{original filename}.nii.gz.`
    - Note: do not move manually since we need the filename to be changed into the standard `{meta id}-{original filename}.nii.gz.`. This is important to notice the pair of matched T1 and T2
4. Moving DICOM Files to be Converted to Niftii
    - The `move2convert()` function handles organizing DICOM files for conversion. Files are moved to a designated conversion folder (`/2convert/`) with sub-directories structured as `{subject-id}-{series-id}-{image-id}`.
    - Current Steps:
        - Compare the metadata with available DICOM files in the source directory `/DICOM/{seq}/{cond}/`
        - Move matching DICOM files to `/2convert/{seq}/{cond}/{subject-id}-{series-id}-{image-id}/`
        - Each file is organized by subject, series, and image IDs for proper tracking during conversion
    - Source code: `move2convert()` function in `data_final_move.ipynb`

5. Moving Non-Preprocessed Files to Preprocessing Queue
    - The `move2preprocess()` function identifies and moves files that haven't been preprocessed yet. These files are organized with sub-directory structure for Windows-based preprocessing (e.g., via MATLAB SPM).
    - Current Steps:
        - Take files from the original directory `/3T/{seq}/{cond}/` based on the metadata list
        - Move files to `/TempData/{seq}/{cond}/{subject-id}-{series-id}-{image-id}/`
        - This organization enables easy transfer to Windows machines for SPM preprocessing
    - Source code: `move2preprocess()` function in `data_final_move.ipynb`

6. Moving Final Preprocessed Files (Free Move)
    - The `freemove()` function provides flexible file movement based on filename patterns, especially useful for moving preprocessed files to the final folder.
    - Current Steps:**
        - Search for preprocessed files using a filename pattern filter (default: `*wm*.nii` - white matter segmented files)
        - Move files from source folder (`/processed/`) to target folder (`/final/`)
        - Files are organized by sequence and condition: `/final/{seq}/{cond}/`
    - Source code: `freemove()` function in `data_final_move.ipynb`

# Data Cleaning of Hold-Out Datasets For Robustness Evaluation
We need new data (hold-out data) that has never been used before during training the models. It is necessary for robustness evaluation.
1. Filtering new data as hold-out dataset
    - Compare the metadata of the training datasets `Balanced_Meta_{seq}w_{group}.csv` with the metadata of the new collected datasets
    - Exclude data that comes from the same subjects / patients (no duplicate of `subject-id`)
    - Copy the data to the folder `/DataPrep`
    - Note: This step can be skipped if the data comes from different repository or database
2. Generate the metadata for the hold-out datasets and separate the data by group/class
    - Move the balanced hold-out data files to the folder `/DataSep/` and put inside a decent sub-directory that contains `{subject id}-{series id}-{image id}`
    - Save the generated metadata inside the folder `/TempMeta/` with the name `HoldOut_Cleaned_T1w_{group}_{Tesla}T.csv`
3. Balance the number of data per class
    - Balancing data per class is done manually by making sure that there is no duplicate subject
    - Move the balanced hold-out data files to the folder `/TempData/` and put inside a decent sub-directory that contains `{subject id}-{series id}-{image id}`
    - The metadata for the balanced hold-out datasets are saved inside the folder `/TempMeta/` with the name `HoldOut_Balanced_T1w_{group}_{Tesla}T.csv`

Source code: `data_final_move.ipynb` -in section Data Separation for Robustness Evaluation

# Python Codebase for Automated Processing

The original Jupyter notebooks have been converted into a modular Python package that automates the data processing pipeline.

## File Operations Functions

The core file movement and organization operations are implemented as reusable Python functions in `src/file_operations.py`. Each function handles a specific step in the data processing pipeline.

### 1. Move Preprocessed Files (`movePreprocessed`)
Collects already-preprocessed files from archive folders and moves them to the target preprocessed directory.

**Purpose**: Handle previously processed data to avoid reprocessing

**Function**:
```python
movePreprocessed(meta_df, path, seq, cond, tesla=3, divider="raw_")
```

**What it does**:
- Searches for preprocessed files matching pattern `**/wm*.nii`
- Compares metadata with available files
- Moves matching files to `/preprocessed/{seq}/{cond}/`
- Exports list of not-yet-preprocessed files as CSV for next step
- Indexes files with metadata ID: `/preprocessed/{seq}/{cond}/{meta-id}-{filename}.nii`

**Inputs**:
- Balanced metadata CSV: `TempMeta/Balanced_Meta_{seq}w_{cond}.csv`
- Source preprocessed files: `./preprocessed_old/{seq}/{cond}/`

**Outputs**:
- Organized preprocessed files: `./preprocessed/{seq}/{cond}/`
- Unprocessed list: `TempMeta/To-Be-Preprocessed_{seq}w_{cond}.csv`

**Usage**:
```bash
python scripts/move_preprocessed_files.py --seq T1 --cond AD --path ./preprocessed_old
```

### 2. Move Files to Preprocessing Queue (`move2preprocess`)
Organizes raw NIFTI files that need preprocessing with proper directory structure for Windows-based processing.

**Purpose**: Prepare files for SPM/MATLAB preprocessing on Windows machines

**Function**:
```python
move2preprocess(meta_df, seq, cond, tesla=3, divider="raw_")
```

**What it does**:
- Identifies files to be preprocessed from metadata
- Organizes each file in a subdirectory: `{subject-id}-{series-id}-{image-id}`
- Moves to `/TempData/{seq}/{cond}/` for transfer to Windows
- Maintains filename integrity for later matching

**File Organization**:
```
TempData/T1/AD/
├── 002_S_0001-S29096-I41124/
│   └── ADNI_002_S_0001_MR_MPRAGE_br_raw_20070329110738780_1_S29096_I41124.nii
├── 002_S_0456-S29097-I41125/
│   └── ADNI_002_S_0456_MR_MPRAGE_br_raw_20070330120442156_2_S29097_I41125.nii
└── ...
```

**Inputs**:
- Unprocessed metadata: `TempMeta/To-Be-Preprocessed_{seq}w_{cond}.csv`
- Raw files: `./3T/{seq}/`

**Outputs**:
- Organized files: `./TempData/{seq}/{cond}/{subject}-{series}-{image}/`

**Usage**:
```bash
python scripts/move_to_preprocess.py --seq T1 --cond AD
```

### 3. Move DICOM Files for Conversion (`move2convert`)
Prepares DICOM files for DICOM-to-NIfTI conversion by organizing them with proper metadata-based directory structure.

**Purpose**: Organize raw DICOM files before conversion process

**Function**:
```python
move2convert(meta_df, seq, cond, tesla=3, divider="raw_")
```

**What it does**:
- Matches DICOM files with metadata
- Creates subdirectories: `{subject-id}-{series-id}_{image-id}`
- Moves to `/2convert/{seq}/{cond}/` for conversion process
- Preserves original DICOM filenames for conversion tools

**File Organization**:
```
2convert/T1/AD/
├── 002_S_0001-S29096_I41124/
│   ├── DICOM_file_001.dcm
│   ├── DICOM_file_002.dcm
│   └── ...
├── 002_S_0456-S29097_I41125/
│   └── ...
└── ...
```

**Notes**:
- DICOM files must be in: `./DICOM/{seq}/{cond}/`
- Directory naming helps conversion tools process related images together
- This step is crucial before DICOM to NIfTI conversion

**Inputs**:
- Balanced metadata CSV: `TempMeta/Balanced_Meta_{seq}w_{cond}.csv`
- DICOM files: `./DICOM/{seq}/{cond}/`

**Outputs**:
- Organized DICOM: `./2convert/{seq}/{cond}/{subject}-{series}_{image}/`

**Usage**:
```bash
python scripts/move_to_convert.py --seq T1 --cond AD
```

### 4. Move Converted Files (`moveConverted`)
Moves converted NIfTI files from conversion folder to preprocessed folder with metadata indexing.

**Purpose**: Archive converted files in preprocessed folder

**Function**:
```python
moveConverted(meta_df, seq, cond, tesla=3, divider="br_")
```

**What it does**:
- Searches for white matter segmented files: `**/wm*.nii`
- Matches metadata with filenames
- Moves to `/preprocessed/{seq}/{cond}/`
- Indexes with metadata ID for later pairing of T1 and T2

**Inputs**:
- Balanced metadata CSV: `TempMeta/Balanced_Meta_{seq}w_{cond}.csv`
- Converted files: `./Converted/{seq}/{cond}/`

**Outputs**:
- Preprocessed files: `./preprocessed/{seq}/{cond}/{meta-id}-{filename}.nii`

### 5. Move Final Processed Files (`freemove`)
Flexible function that moves any files based on filename pattern matching.

**Purpose**: Move final preprocessed/processed files to output directory

**Function**:
```python
freemove(source_path, target_path, seq, cond, tesla=3, file_format='**/*wm*.nii')
```

**What it does**:
- Uses customizable glob pattern to find files
- Default pattern matches white matter segmented files: `**/wm*.nii`
- Moves to `/final/{seq}/{cond}/` with indexed naming
- Flexible for different file types and patterns

**File Organization**:
```
final/T1/AD/
├── 0-wm{filename}.nii
├── 1-wm{filename}.nii
├── 2-wm{filename}.nii
└── ...
```

**Parameters**:
- `file_format`: Glob pattern (e.g., `**/*wm*.nii`, `**/*y_*.nii`, `**/*p*.nii`)
- Can move different file types: segmented, normalized, warped, etc.

**Inputs**:
- Processed files: `./processed/{seq}/{cond}/`

**Outputs**:
- Final files: `./final/{seq}/{cond}/{index}-{filename}.nii`

**Usage**:
```bash
# Move white matter segmented files (default)
python scripts/move_final_files.py --seq T1 --cond AD \
    --source ./processed --target ./final

# Move normalized files
python scripts/move_final_files.py --seq T1 --cond AD \
    --source ./processed --target ./final --pattern "**/*n*.nii"

# Move warped files
python scripts/move_final_files.py --seq T1 --cond AD \
    --source ./processed --target ./final --pattern "**/*wn*.nii"
```

### 6. Separate Data for Robustness Evaluation (`move2separate`)
Organizes hold-out datasets for robustness evaluation with proper grouping.

**Purpose**: Prepare new/hold-out data that has never been used in training

**Function**:
```python
move2separate(meta_df, seq, tesla=3, ONLY_BASELINE=False, divider="Br_")
```

**What it does**:
- Filters new datasets from different sources
- Excludes data from subjects already in training set
- Organizes into `/DataSep/` for manual verification
- Enables robustness testing with completely new data

**Inputs**:
- New dataset metadata: `HoldOut_Cleaned_{seq}w_{cond}_{tesla}T.csv`
- New data files: source directory

**Outputs**:
- Separated data: `./DataSep/{seq}/{subject-id}-{series-id}/`

## Master Pipeline Orchestrator

Run the complete workflow automatically:

```bash
python scripts/run_pipeline.py --seq T1 --cond AD --step all
```

**Pipeline Steps**:
1. Move preprocessed files
2. Move files to preprocessing queue
3. Move DICOM files for conversion
4. Move final processed files

**Options**:
```
--seq {T1, T2}              # MRI sequence (required)
--cond {AD, CN, MCI}        # Condition (required)
--step {all|step_name}      # Which step to run (default: all)
--old-path PATH             # Path to old preprocessed files
--source-path PATH          # Path to processed files
--target-path PATH          # Output path for final files
```

**Examples**:
```bash
# Process single group
python scripts/run_pipeline.py --seq T1 --cond AD --step all

# Run only preprocessing step
python scripts/run_pipeline.py --seq T1 --cond AD --step move_to_preprocess

# Process multiple groups (T1)
for cond in AD CN MCI; do
    python scripts/run_pipeline.py --seq T1 --cond $cond --step all
done

# Process all groups (T1 and T2)
for seq in T1 T2; do
    for cond in AD CN MCI; do
        python scripts/run_pipeline.py --seq $seq --cond $cond --step all
    done
done
```

## Logging and Output

All operations are logged to `outputs/logs/` with timestamps and details:

```
outputs/logs/
├── move_preprocessed_files_20241229_101523.log
├── move_to_preprocess_20241229_101545.log
├── move_to_convert_20241229_101612.log
└── move_final_files_20241229_101634.log
```

Log example:
```
2024-12-29 10:15:23,456 - move_preprocessed_files - INFO - Starting: Move Preprocessed Files
2024-12-29 10:15:24,123 - move_preprocessed_files - INFO - Loaded metadata with 150 records
2024-12-29 10:15:45,789 - move_preprocessed_files - INFO - Total T1w-AD data is 145 and not preprocessed is 5
2024-12-29 10:15:45,456 - move_preprocessed_files - INFO - Completed Move Preprocessed Files in 21.57s
```

## Configuration

Global configuration in `src/config.py`:
- Directory paths (inputs, outputs, temporary)
- Sequences, conditions, Tesla strengths
- File patterns and delimiters
- Metadata column definitions
- Logging settings

Customize these settings for different project configurations.

# Data Preprocessing
Data preprocessing is performed using SPM, which includes the following process:
1. Data normalization / Intensity normalization
2. Skull-stripping
3. Data registration (matching to one template and put the brain in the standardized space)
4. Data scaling

# Data Augmentation
Due to unstable performance of the Deep Learning model, we have implemented data augmentation techniques to AD and other classes too in order to increase number of training and testing data. In this case, we added more images up to 300 data. 
The implemented data augmentation techniques include flipping and rotation. Flipping is applied to all 150 volume images of non AD group in T1 and T2 as well as to the volume images in AD group. All augmented images by flipping technique have been saved with the name `flip_filename`. Meanwhile, we also applied rotation only to AD group. 5 degree rotation is applied to the first half (25) dataset and -5 degree rotation is applied to the rest half dataset of AD group in T1 and T2. As we still need more data, we applied more rotation and flipping techniques to the AD group in T1 and T2. Thus, there are 5 kinds of data additions in AD group, which are `flip`, `rotP`, `rotM`, `rotP_flip`, `rotM_flip`. Therefore, eventually we obtained 300 volume images in AD group.

# Data Loading

Data loading is another important step in this experiment. During the data loading, we also need to make sure that the pairs of T1 and T2 data are fed together to the same model for the joint training technique. Besides, as we have all known, in order to prevent data leakage, data augmentation should be done after data splitting. However, in this experiment, we have first executed data augmentation. Since we want to do 5-fold cross validation and our data splitting is based on 70/10/20 separation for train/valid/test, we must build data loader that will load the data as if we performed proper data splitting (split the data before augmentation). To do that, we can make use of the `meta-id` in the file name. 

In this case, we have created the data loading algorithm for proper data splitting as presented in section `Load Dataset` in the codes. The codes are available in the main repository.

# About Citation
For more detail please refer to the publication: 

[1] Rumala, D.J. (2023). How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis. In: Wesarg, S., et al. Clinical Image-Based Procedures, Fairness of AI in Medical Imaging, and Ethical and Philosophical Issues in Medical Imaging . CLIP EPIMI FAIMI 2023 2023 2023. Lecture Notes in Computer Science, vol 14242. Springer, Cham. [https://doi.org/10.1007/978-3-031-45249-9_23](https://doi.org/10.1007/978-3-031-45249-9_23).

More information and comprehensive summary about this publication can be freely accessed here: [https://djrumala.github.io/publications/how-you-split-matters](https://djrumala.github.io/publications/how-you-split-matters)
